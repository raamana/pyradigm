{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Table of Contents\n",
        "\n",
        "* <a href='#motivation'>Motivation</a>\n",
        "\n",
        "* <a href='#constructor'>Constructing a dataset</a>\n",
        "\n",
        "* <a href='#attributes'>Attributes</a>\n",
        "\n",
        "* <a href='#access'>Accessing samples</a>\n",
        "\n",
        "* <a href='#iteration'>Iteration over samples</a>\n",
        "\n",
        "* <a href='#subsetselection'>Subset selection</a>\n",
        "\n",
        "* <a href='#serialization'>Saving/reloading a dataset (Serialization)</a>\n",
        "\n",
        "* <a href='#arithmetic'> Combining multiple datasets and arithmetic on useful subsets within datasets </a>\n",
        "\n* <a href='#portability'>Portability (e.g. with sklearn)</a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='motivation'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Python data structure to improve handling of datasets in machine learning workflows\n",
        "\n",
        "This class is greatly suited for neuroimaging applications (or any other domain), where each sample needs to be uniquely identified with a subject ID (or something similar). \n",
        "\n",
        "Key-level correspondence across data, labels (1 or 2), classnames ('healthy', 'disease') and the related helps maintain data integrity and improve the provenance, in addition to enabling traceback to original sources from where the features have been originally derived.\n",
        "\n",
        "Just to given you a concrete examples, let's look at how an ML dataset is handled traditionally.\n",
        "\n",
        "You have a matrix X of size n x p, with n samples and p features, and a vector y containing the target values (or class labels or class identifiers). This X and y serves as training (and test set) for a classifier like SVM to fit the data X to match y as accurately as possible.\n",
        "\nLet's get a little more concrete:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "%matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10 # number of samples\n",
        "p = 3  # number of features\n",
        "\n",
        "X = np.random.random([n, p]) # random data for illustration\n",
        "y = [1]*5 + [2]*5            # random labels ...\n",
        "\n",
        "np.set_printoptions(precision=2) # save some screen space\n",
        "print('X : \\n{}'.format(X))\n",
        "print('y : \\n{}'.format(y))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using matplotlib backend: TkAgg\n",
            "X : \n",
            "[[ 0.73  0.85  0.3 ]\n",
            " [ 0.63  0.09  0.87]\n",
            " [ 0.14  0.71  0.19]\n",
            " [ 0.25  0.33  0.08]\n",
            " [ 0.8   0.85  0.99]\n",
            " [ 0.78  0.76  0.47]\n",
            " [ 0.25  0.54  0.18]\n",
            " [ 0.57  0.98  0.36]\n",
            " [ 0.1   0.1   0.74]\n",
            " [ 0.16  0.76  0.53]]\n",
            "y : \n",
            "[1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all the machine learning toolboxes take their input in this form: X and y, regardless of the original source that produced these features in the first place.\n",
        "\n",
        "This is all fine if all you ever wanted to do is to extract some features, do some machine learning and dispose these features away! \n",
        "\n",
        "** But this is almost never the case!**\n",
        "\n",
        "Because it doesn't simply end there.\n",
        "\n",
        "At a minimum, I often need to know \n",
        " * which samples are misclassified - meaning you need to know what the identifiers are and not simply their row indices in X?\n",
        " * what are the characteristics of those samples?\n",
        " * what classes do they belong to?\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And all this info needs to be obtained\n",
        " * without having to write lots of code connecting few non-obvious links to disparate sources of data (numerical features X, and sample identifiers in a CSV file) to find the relevant info\n",
        " * without having to track down who or which method originally produced these features\n",
        " * how the previous personnel or grad student organized the whole dataset, if you haven't generated the features yourself from scratch\n",
        "\n",
        "And if you are like me, you would be thinking about how would you organize your workflow such that the aforementioned tasks can be accomplished with ease.\n",
        " \n",
        "This data structure attempts to accomplish that with ease. By always organizing the extracted features keyed-in into a dictionary with their *sample id*, and other important info such as *target values* and other identified info. This, by definition, preserves the integrity of the data (inability to incorrectly label samples etc).\n",
        "\nNo, this data structure doesn't offer the full [provenance tracking](http://rrcns.readthedocs.io/en/latest/provenance_tracking.html), which is quite a challenging problem. But it tries make your life a little easier in your ML workflows."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example application is shown below, touching upon the following topics:\n",
        "\n",
        "* <a href='#motivation'>Motivation</a>\n",
        "\n",
        "* <a href='#constructor'>Constructing a dataset</a>\n",
        "\n",
        "* <a href='#attributes'>Attributes</a>\n",
        "\n",
        "* <a href='#access'>Accessing samples</a>\n",
        "\n",
        "* <a href='#iteration'>Iteration over samples</a>\n",
        "\n",
        "* <a href='#subsetselection'>Subset selection</a>\n",
        "\n",
        "* <a href='#serialization'>Saving/reloading a dataset (Serialization)</a>\n",
        "\n",
        "* <a href='#arithmetic'> Combining multiple datasets and arithmetic on useful subsets within datasets </a>\n",
        "\n* <a href='#portability'>Portability (e.g. with sklearn)</a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improting the necessary modules and our fancy class definition:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyradigm import MLDataset"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now instantiate it and give it a description:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MLDataset()\n",
        "dataset.description = 'ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.'"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": [
              "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "Empty dataset."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the dataset some description attached to it, however we know it is empty. This can be verified in a boolean context as shown below:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bool(dataset)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add samples to this dataset which is when this dataset implementation becomes really handy. Before we do that, we will define some convenience routines defined to just illustrate a simple yet common use of this dataset."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_thickness(path):\n",
        "    \"\"\"Dummy function to minic a data reader.\"\"\"\n",
        "\n",
        "    # in your actural routine, this might be:\n",
        "    #   pysurfer.read_thickness(path).values()\n",
        "    return np.random.random(2)\n",
        "\n\n",
        "def get_features(work_dir, subj_id):\n",
        "    \"\"\"Returns the whole brain cortical thickness for a given subject ID.\"\"\"\n",
        "\n",
        "    # extension to identify the data file; this could be .curv, anything else you choose\n",
        "    ext_thickness = '.thickness'\n",
        "\n",
        "    thickness = dict()\n",
        "    for hemi in ['lh', 'rh']:\n",
        "        path_thickness = os.path.join(work_dir, subj_id, hemi + ext_thickness)\n",
        "        thickness[hemi] = read_thickness(path_thickness)\n",
        "\n",
        "    # concatenating them to build a whole brain feature set\n",
        "    thickness_wb = np.concatenate([thickness['lh'], thickness['rh']])\n",
        "\n    return thickness_wb"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we have IO routines to read the data for us. Let's define where the data will come from:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "work_dir = '/project/ADNI/FreesurferThickness_v4p3'\n",
        "class_set = ['Cntrl', 'Alzmr', 'MCI']\n",
        "class_sizes = [15, 12, 18]"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This would obviously change for your applications, but this has sufficient properties to illustrate the point."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what methods this dataset offers us:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dir(dataset)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": [
              "['add_classes',\n",
              " 'add_sample',\n",
              " 'class_set',\n",
              " 'class_sizes',\n",
              " 'classes',\n",
              " 'data',\n",
              " 'data_and_labels',\n",
              " 'del_sample',\n",
              " 'description',\n",
              " 'extend',\n",
              " 'feature_names',\n",
              " 'get_class',\n",
              " 'get_feature_subset',\n",
              " 'get_subset',\n",
              " 'glance',\n",
              " 'keys',\n",
              " 'num_classes',\n",
              " 'num_features',\n",
              " 'num_samples',\n",
              " 'random_subset',\n",
              " 'random_subset_ids',\n",
              " 'random_subset_ids_by_count',\n",
              " 'sample_ids',\n",
              " 'sample_ids_in_class',\n",
              " 'save',\n",
              " 'summarize_classes',\n",
              " 'train_test_split_ids',\n",
              " 'transform']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a lot of methods of convenience to organize and retrieve dataset. \n",
        "\nSo let's go through them by their usage sections."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='constructor'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructor\n",
        "\nYou can see there few methods such as `add_sample`, `get_subset` etc: important method being `add_sample`, which is key to constructing this dataset. Let's go ahead and some samples:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "To contruct a dataset, one typically starts with a list of subject IDs to be added - we create few random lists, each to be considered as a separate class:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "random.seed(datetime.now())\n",
        "\n",
        "def read_target_list(class_name, class_size):\n",
        "    \"Generates a random target list. In reality, you would do something like the commented code below.\"\n",
        "    target_list = list()\n",
        "    for idx in range(class_size):\n",
        "        target_list.append('{}{:04d}'.format(class_name[0],np.random.randint(1000)))\n",
        "        \n",
        "    return target_list\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we go through each of the above classes, and add each sample that class to the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for class_index, class_id in enumerate(class_set):\n",
        "    print('Working on class {:>5}'.format(class_id))\n",
        "\n",
        "    target_list = read_target_list(class_id,class_sizes[class_index])\n",
        "    for subj_id in target_list:\n",
        "        print('\\t reading subject {:>15}'.format(subj_id))\n",
        "        thickness_wb = get_features(work_dir, subj_id)\n",
        "\n",
        "        # adding the sample to the dataset\n",
        "        dataset.add_sample(subj_id, thickness_wb, class_index, class_id)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on class Cntrl\n",
            "\t reading subject           C0562\n",
            "\t reading subject           C0408\n",
            "\t reading subject           C0760\n",
            "\t reading subject           C0170\n",
            "\t reading subject           C0241\n",
            "\t reading subject           C0980\n",
            "\t reading subject           C0822\n",
            "\t reading subject           C0565\n",
            "\t reading subject           C0949\n",
            "\t reading subject           C0041\n",
            "\t reading subject           C0372\n",
            "\t reading subject           C0141\n",
            "\t reading subject           C0492\n",
            "\t reading subject           C0064\n",
            "\t reading subject           C0557\n",
            "Working on class Alzmr\n",
            "\t reading subject           A0034\n",
            "\t reading subject           A0768\n",
            "\t reading subject           A0240\n",
            "\t reading subject           A0042\n",
            "\t reading subject           A0141\n",
            "\t reading subject           A0888\n",
            "\t reading subject           A0032\n",
            "\t reading subject           A0596\n",
            "\t reading subject           A0969\n",
            "\t reading subject           A0215\n",
            "\t reading subject           A0074\n",
            "\t reading subject           A0229\n",
            "Working on class   MCI\n",
            "\t reading subject           M0760\n",
            "\t reading subject           M0434\n",
            "\t reading subject           M0033\n",
            "\t reading subject           M0942\n",
            "\t reading subject           M0034\n",
            "\t reading subject           M0868\n",
            "\t reading subject           M0595\n",
            "\t reading subject           M0476\n",
            "\t reading subject           M0770\n",
            "\t reading subject           M0577\n",
            "\t reading subject           M0638\n",
            "\t reading subject           M0421\n",
            "\t reading subject           M0006\n",
            "\t reading subject           M0552\n",
            "\t reading subject           M0040\n",
            "\t reading subject           M0165\n",
            "\t reading subject           M0256\n",
            "\t reading subject           M0127\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nice. Isn't it?**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what's nice about this, you say? *The simple fact that you are constructing a dataset as you read the data* in its most elemental form (in the units of the dataset such as the subject ID in our neuroimaging application). You're done as soon as you're done reading the features from disk.\n",
        "\nWhat's more - you can inspect the dataset in an intuitive manner, as shown below:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": [
              "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "45 samples, 3 classes, 4 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples.\n",
              "Class   MCI : 18 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even better, right? No more coding of several commands to get the complete and concise sense of the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='attributes'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convenient attributes\n",
        "\nIf you would like, you can always get more specific information, such as:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_samples"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.num_features"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.class_set"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": [
              "['MCI', 'Cntrl', 'Alzmr']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.class_sizes"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": [
              "Counter({'Alzmr': 12, 'Cntrl': 15, 'MCI': 18})"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.class_sizes['Cntrl']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd like to take a look data inside for few subjects - shall we call it a glance?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.glance()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": [
              "{'C0170': array([ 0.37,  0.78,  0.5 ,  0.79]),\n",
              " 'C0241': array([ 0.11,  0.18,  0.58,  0.36]),\n",
              " 'C0408': array([ 0.49,  0.38,  0.05,  0.82]),\n",
              " 'C0562': array([ 0.64,  0.59,  0.01,  0.8 ]),\n",
              " 'C0760': array([ 0.12,  0.51,  0.95,  0.23])}"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can control the number of items to glance, by passing a number to dataset.glance() method:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.glance(2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": [
              "{'C0408': array([ 0.49,  0.38,  0.05,  0.82]),\n",
              " 'C0562': array([ 0.64,  0.59,  0.01,  0.8 ])}"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or you may be wondering what are the subject IDs in the dataset.. here they are:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.sample_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": [
              "['C0562',\n",
              " 'C0408',\n",
              " 'C0760',\n",
              " 'C0170',\n",
              " 'C0241',\n",
              " 'C0980',\n",
              " 'C0822',\n",
              " 'C0565',\n",
              " 'C0949',\n",
              " 'C0041',\n",
              " 'C0372',\n",
              " 'C0141',\n",
              " 'C0492',\n",
              " 'C0064',\n",
              " 'C0557',\n",
              " 'A0034',\n",
              " 'A0768',\n",
              " 'A0240',\n",
              " 'A0042',\n",
              " 'A0141',\n",
              " 'A0888',\n",
              " 'A0032',\n",
              " 'A0596',\n",
              " 'A0969',\n",
              " 'A0215',\n",
              " 'A0074',\n",
              " 'A0229',\n",
              " 'M0760',\n",
              " 'M0434',\n",
              " 'M0033',\n",
              " 'M0942',\n",
              " 'M0034',\n",
              " 'M0868',\n",
              " 'M0595',\n",
              " 'M0476',\n",
              " 'M0770',\n",
              " 'M0577',\n",
              " 'M0638',\n",
              " 'M0421',\n",
              " 'M0006',\n",
              " 'M0552',\n",
              " 'M0040',\n",
              " 'M0165',\n",
              " 'M0256',\n",
              " 'M0127']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='access'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing samples\n",
        "\nThanks to elegant implementation, data for a given sample 'M0299' can simply be obtained by:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['M0040']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": [
              "array([ 0.27,  0.52,  0.61,  0.49])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like a Python dict, it raises an error if the key is not in the dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['dlfjdjf']"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'dlfjdjf not found in dataset.'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4b19d52bac71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dlfjdjf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/dev/pyradigm/pyradigm/pyradigm.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} not found in dataset.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dlfjdjf not found in dataset.'"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more graceful handling would be to use `dataset.get` to control what value to be returned in case the requested id is not found in the dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.get('dkfjd', np.nan)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='iteration'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteration"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to builtin iteration, we can easily iterate over all the samples:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for sample, features in dataset:\n",
        "    print(\"{} : {:>10} : {}\".format(sample, dataset.classes[sample], features))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C0562 :      Cntrl : [ 0.64  0.59  0.01  0.8 ]\n",
            "C0408 :      Cntrl : [ 0.49  0.38  0.05  0.82]\n",
            "C0760 :      Cntrl : [ 0.12  0.51  0.95  0.23]\n",
            "C0170 :      Cntrl : [ 0.37  0.78  0.5   0.79]\n",
            "C0241 :      Cntrl : [ 0.11  0.18  0.58  0.36]\n",
            "C0980 :      Cntrl : [ 0.1   0.52  0.79  0.68]\n",
            "C0822 :      Cntrl : [ 0.44  0.97  0.06  0.99]\n",
            "C0565 :      Cntrl : [ 0.89  0.5   0.89  0.48]\n",
            "C0949 :      Cntrl : [ 0.84  0.84  0.51  0.12]\n",
            "C0041 :      Cntrl : [ 0.07  0.19  0.68  0.81]\n",
            "C0372 :      Cntrl : [ 0.7   0.05  0.67  0.39]\n",
            "C0141 :      Cntrl : [ 0.46  0.18  0.69  0.17]\n",
            "C0492 :      Cntrl : [ 0.82  0.77  0.07  0.69]\n",
            "C0064 :      Cntrl : [ 0.24  0.54  0.36  0.37]\n",
            "C0557 :      Cntrl : [ 0.59  0.86  0.1   0.42]\n",
            "A0034 :      Alzmr : [ 0.35  0.96  0.41  0.93]\n",
            "A0768 :      Alzmr : [ 0.65  0.37  0.7   0.24]\n",
            "A0240 :      Alzmr : [ 0.87  0.78  0.1   0.28]\n",
            "A0042 :      Alzmr : [ 0.12  0.3   0.35  0.7 ]\n",
            "A0141 :      Alzmr : [ 0.85  0.28  0.06  0.74]\n",
            "A0888 :      Alzmr : [ 0.85  0.78  0.93  0.7 ]\n",
            "A0032 :      Alzmr : [ 0.28  0.41  0.61  0.09]\n",
            "A0596 :      Alzmr : [ 0.28  0.15  0.88  0.23]\n",
            "A0969 :      Alzmr : [ 0.47  0.37  0.52  0.58]\n",
            "A0215 :      Alzmr : [ 0.49  0.7   0.31  0.96]\n",
            "A0074 :      Alzmr : [ 0.87  0.7   0.37  0.7 ]\n",
            "A0229 :      Alzmr : [ 0.96  0.34  0.59  0.96]\n",
            "M0760 :        MCI : [ 0.27  0.22  0.37  0.14]\n",
            "M0434 :        MCI : [ 0.26  0.04  0.49  0.92]\n",
            "M0033 :        MCI : [ 0.14  0.39  0.71  0.5 ]\n",
            "M0942 :        MCI : [ 0.19  0.29  0.42  0.46]\n",
            "M0034 :        MCI : [ 0.36  0.54  0.67  0.71]\n",
            "M0868 :        MCI : [ 0.29  0.46  0.47  0.83]\n",
            "M0595 :        MCI : [ 0.62  0.07  0.66  0.75]\n",
            "M0476 :        MCI : [ 0.73  0.97  0.59  0.24]\n",
            "M0770 :        MCI : [ 0.81  0.78  0.28  0.61]\n",
            "M0577 :        MCI : [ 0.84  0.86  0.94  0.5 ]\n",
            "M0638 :        MCI : [ 0.61  0.64  0.94  0.94]\n",
            "M0421 :        MCI : [ 0.73  0.16  0.97  0.69]\n",
            "M0006 :        MCI : [ 0.76  0.62  0.49  0.03]\n",
            "M0552 :        MCI : [ 0.26  0.85  0.13  0.31]\n",
            "M0040 :        MCI : [ 0.27  0.52  0.61  0.49]\n",
            "M0165 :        MCI : [ 0.03  0.79  0.92  0.79]\n",
            "M0256 :        MCI : [ 0.06  0.06  0.69  0.97]\n",
            "M0127 :        MCI : [ 0.42  0.11  0.93  0.5 ]\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did you see that? *It's so intuitive and natural!* Such a clean traversal of dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to the choice of the OrderedDict() to represent the data, classes and labels underneath, the order of sample addition is retained. Hence the correspondence across samples in the dataset not only key-wise (by the sample id), but also index-wise."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='transform'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subject-wise transform\n",
        "\n",
        "Quite often, we are interested in computing some statistics on data for a given subject (such as mean, or ROI-wise median). Typically this requires a loop, with some computation and organizing it in a new dataset! A simple routine pattern of usage, but can't avoided if you are still fiddling with representing your dataset in medieval matrices! :).\n",
        "\nIf you organized your dataset in a `pyradigm`, such computation is trivial, thanks to builtin implementation of `transform` method. The mean value for each subject can be computed and organized in a new dataset, with an intuitive and single line:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mean_data = dataset.transform(np.mean)\n",
        "mean_data.description = 'mean values per subject'\n",
        "mean_data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": [
              "mean values per subject\n",
              "45 samples, 3 classes, 1 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples.\n",
              "Class   MCI : 18 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the transform accepts an arbitrary callable, we could do many more sophisticated things, such as access the subset of features e.g. cortical thickness for a particular region of interest (say posterior cingulate gyrus)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make a toy function to return the indices for the ROI\n",
        "def get_ROI_indices(x): return x[:3] "
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this \"mask\" function, we can easily obtain features for an ROI"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pcg = dataset.transform(get_ROI_indices)"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can verify that the new dataset does indeed have only 3 features, for the same subjects/classes:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pcg"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": [
              "None\n",
              "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "45 samples, 3 classes, 3 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples.\n",
              "Class   MCI : 18 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pcg.num_features"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a bar plot with the just computed numbers:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data, lbl, keys = pcg.data_and_labels()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n, bins, patches = plt.hist(data)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
              "AAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\n",
              "dHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADEJJREFUeJzt3W+IZXd5wPHvY8Z0GxtX6U6hJBln\n",
              "BZUumReGwcYK/ouUmJTkhUHWEruW2EGL0baCbBFx0TcRVFQItIvartWqNYosXfvfhFBptu4mqZPs\n",
              "qsS4javRxLauhVKT4OOLe9V1nbnnt+Occ+e58/3AwL1zzt77nHvvfPfMmfsnMhNJUh1PmvYAkqTz\n",
              "Y7glqRjDLUnFGG5JKsZwS1IxhluSijHcklSM4ZakYgy3JBUz18eF7tq1KxcXF/u4aEmaScePH/9u\n",
              "Zs63rNtLuBcXFzl27FgfFy1JMyki/rN1XQ+VSFIxhluSijHcklSM4ZakYgy3JBVjuCWpGMMtScUY\n",
              "bkkqxnBLUjG9vHJSkmbR0qGlictX960OMod73JJUjOGWpGIMtyQVY7glqRjDLUnFGG5JKsZwS1Ix\n",
              "hluSijHcklSM4ZakYgy3JBVjuCWpGMMtScUYbkkqxnBLUjGGW5KKMdySVIzhlqRimsIdEX8cEfdH\n",
              "xH0R8fGI2NH3YJKktXWGOyIuAd4ILGfm5cAFwN6+B5Mkra31UMkc8MsRMQdcBHyrv5EkSZN0hjsz\n",
              "vwm8G3gIeBg4k5n/2PdgkqS1zXWtEBFPB64HdgPfAz4VETdm5kfPWW8FWAFYWFjoYdTZtrj/yMTl\n",
              "p265dqBJVMXSoaWJy1f3rQ40Sbuuxzn4WG/RcqjkZcDXM/PRzHwc+AzwW+eulJkHM3M5M5fn5+c3\n",
              "e05J0lhLuB8CroyIiyIigKuAk/2OJUlaT8sx7qPAbcDdwOr43xzseS5J0jo6j3EDZObbgbf3PIsk\n",
              "qYGvnJSkYgy3JBVjuCWpGMMtScUYbkkqxnBLUjGGW5KKMdySVIzhlqRiDLckFWO4JakYwy1JxRhu\n",
              "SSrGcEtSMYZbkoox3JJUjOGWpGIMtyQV0/TRZZUsHVrqXGd13+oAk0hSP9zjlqRiDLckFWO4JakY\n",
              "wy1JxRhuSSrGcEtSMYZbkoox3JJUjOGWpGIMtyQVY7glqRjDLUnFGG5JKsZwS1IxhluSijHcklSM\n",
              "4ZakYgy3JBXTFO6IeFpE3BYRX46IkxHx/L4HkyStrfUzJ98P/H1m3hARFwIX9TiTJGmCznBHxE7g\n",
              "hcBrADLzMeCxfseSJK2n5VDJbuBR4C8i4p6I+GBEPKXnuSRJ62g5VDIHXAHcnJlHI+L9wH7gbWev\n",
              "FBErwArAwsLCZs+5NRzY2bH8zDBzaBhd9zfM5n0+7e3256xTyx73aeB0Zh4dn7+NUch/RmYezMzl\n",
              "zFyen5/fzBklSWfpDHdmfhv4RkQ8Z/ytq4ATvU4lSVpX67NKbgY+Nn5GyYPA7/c3kiRpkqZwZ+a9\n",
              "wHLPs0iSGvjKSUkqxnBLUjGGW5KKMdySVIzhlqRiDLckFWO4JakYwy1JxRhuSSrGcEtSMYZbkoox\n",
              "3JJUjOGWpGIMtyQVY7glqRjDLUnFGG5JKsZwS1IxrZ85uS0s7j8ycfmpHQMNstUc2Nm5ytLuhYnL\n",
              "V/etbtY0P3/dh5amdt0b1flYu+XagSZRRe5xS1IxhluSijHcklSM4ZakYgy3JBVjuCWpGMMtScUY\n",
              "bkkqxnBLUjGGW5KKMdySVIzhlqRiDLckFWO4JakYwy1JxRhuSSrGcEtSMYZbkoppDndEXBAR90TE\n",
              "3/Y5kCRpsvPZ434TcLKvQSRJbZrCHRGXAtcCH+x3HElSl9Y97vcBbwF+2OMskqQGc10rRMTvAI9k\n",
              "5vGIePGE9VaAFYCFhYUND7S4/8jE5ad2/O7kC9i98eve0g7s7Fh+Zpg5Nlnn/X3LtQNNsjFLh5Ym\n",
              "Ll/dtzrQJO26f8a6L6Pids+Slj3uFwDXRcQp4BPASyPio+eulJkHM3M5M5fn5+c3eUxJ0o91hjsz\n",
              "/zQzL83MRWAv8PnMvLH3ySRJa/J53JJUTOcx7rNl5h3AHb1MIklq4h63JBVjuCWpGMMtScUYbkkq\n",
              "xnBLUjGGW5KKMdySVIzhlqRiDLckFWO4JakYwy1JxRhuSSrGcEtSMYZbkoox3JJUjOGWpGIMtyQV\n",
              "Y7glqZjz+ugyzabF/UcmLj+1Y6BB9FMHdnavs3uh/zm2m67bfYvc5u5xS1IxhluSijHcklSM4Zak\n",
              "Ygy3JBVjuCWpGMMtScUYbkkqxnBLUjGGW5KKMdySVIzhlqRiDLckFWO4JakYwy1JxRhuSSrGcEtS\n",
              "MYZbkorpDHdEXBYRt0fEiYi4PyLeNMRgkqS1tXzm5BPAmzPz7oi4GDgeEf+UmSd6nk2StIbOPe7M\n",
              "fDgz7x6f/l/gJHBJ34NJktZ2Xse4I2IReC5wtI9hJEndWg6VABARvwJ8GvijzPz+GstXgBWAhYWt\n",
              "8RH2mgEHdnavs3tjj7fF/UcmLj+1Y0MXqy2q6/6GOvd50x53RDyZUbQ/lpmfWWudzDyYmcuZuTw/\n",
              "P7+ZM0qSztLyrJIAPgSczMz39j+SJGmSlj3uFwCvBl4aEfeOv67peS5J0jo6j3Fn5r8CMcAskqQG\n",
              "vnJSkoox3JJUjOGWpGIMtyQVY7glqRjDLUnFGG5JKsZwS1IxhluSijHcklSM4ZakYgy3JBVjuCWp\n",
              "GMMtScUYbkkqxnBLUjGGW5KKMdySVEznR5ep3dKhpc51Vvetztx1S0Pqeqxvh8e5e9ySVIzhlqRi\n",
              "DLckFWO4JakYwy1JxRhuSSrGcEtSMYZbkoox3JJUjOGWpGIMtyQVY7glqRjDLUnFGG5JKsZwS1Ix\n",
              "hluSijHcklSM4ZakYprCHRFXR8RXIuKBiNjf91CSpPV1hjsiLgBuBV4O7AFeFRF7+h5MkrS2lj3u\n",
              "5wEPZOaDmfkY8Ang+n7HkiStpyXclwDfOOv86fH3JElTEJk5eYWIG4CrM/O14/OvBn4zM99wznor\n",
              "wMr47HOAr3Rc9y7guxsZekZs9+0HbwO33+0/e/ufkZnzLf9wrmGdbwKXnXX+0vH3fkZmHgQOtlwp\n",
              "QEQcy8zl1vVnzXbffvA2cPvd/o1uf8uhki8Cz4qI3RFxIbAXOLyRK5Mk/eI697gz84mIeAPwD8AF\n",
              "wIcz8/7eJ5MkranlUAmZ+Tngc5t83c2HVWbUdt9+8DZw+7e3DW9/5x8nJUlbiy95l6Rieg9318vl\n",
              "I+KXIuKT4+VHI2Kx75mG1LD9fxIRJyLiSxHxLxHxjGnM2ZfWt0uIiFdEREbETD3LoGX7I+KV48fA\n",
              "/RHx10PP2LeGn4GFiLg9Iu4Z/xxcM405+xARH46IRyLivnWWR0R8YHzbfCkirmi64Mzs7YvRHzO/\n",
              "BjwTuBD4D2DPOev8IfBn49N7gU/2OdOQX43b/xLgovHp12+37R+vdzFwJ3AXsDztuQe+/58F3AM8\n",
              "fXz+16Y99xRug4PA68en9wCnpj33Jm7/C4ErgPvWWX4N8HdAAFcCR1sut+897paXy18PHBqfvg24\n",
              "KiKi57mG0rn9mXl7Zv7f+OxdjJ4nPyta3y7hncC7gP8fcrgBtGz/HwC3Zub/AGTmIwPP2LeW2yCB\n",
              "p45P7wS+NeB8vcrMO4H/nrDK9cBHcuQu4GkR8etdl9t3uFteLv+TdTLzCeAM8Ks9zzWU8327gJsY\n",
              "/e87Kzq3f/yr4WWZeWTIwQbScv8/G3h2RHwhIu6KiKsHm24YLbfBAeDGiDjN6NlrNw8z2pawobcU\n",
              "aXo6oPoXETcCy8CLpj3LUCLiScB7gddMeZRpmmN0uOTFjH7bujMiljLze1OdalivAv4yM98TEc8H\n",
              "/ioiLs/MH057sK2q7z3ulpfL/2SdiJhj9KvSf/U811Ca3i4gIl4GvBW4LjN/MNBsQ+ja/ouBy4E7\n",
              "IuIUo2N8h2foD5Qt9/9p4HBmPp6ZXwe+yijks6LlNrgJ+BuAzPw3YAej9/HYDpoaca6+w93ycvnD\n",
              "wL7x6RuAz+f4qP0M6Nz+iHgu8OeMoj1rxzcnbn9mnsnMXZm5mJmLjI7xX5eZx6Yz7qZrefx/ltHe\n",
              "NhGxi9GhkweHHLJnLbfBQ8BVABHxG4zC/eigU07PYeD3xs8uuRI4k5kPd/6rAf6qeg2jvYivAW8d\n",
              "f+8djH5AYXQnfQp4APh34JnT/kvwwNv/z8B3gHvHX4enPfOQ23/OuncwQ88qabz/g9HhohPAKrB3\n",
              "2jNP4TbYA3yB0TNO7gV+e9ozb+K2fxx4GHic0W9XNwGvA1531v1/6/i2WW19/PvKSUkqxldOSlIx\n",
              "hluSijHcklSM4ZakYgy3JBVjuCWpGMMtScUYbkkq5kfgzt1Pz0u3ZQAAAABJRU5ErkJggg==\n"
            ],
            "text/plain": [
              "<matplotlib.figure.Figure at 0x1138d6390>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember as the original source of data was random, this has no units, property or meaning!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='subsetselection'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subset selection\n",
        "\nIn addition to the structured way of obtaining the various properties of this dataset, this implementation really will come in handy when you have to slice and dice the dataset (with large number of classes and features) into smaller subsets (e.g. for binary classification). Let's see how we can retrieve the data for a single class:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl = dataset.get_class('Cntrl')"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it, obtaining the data for a given class is a simple call away.\n",
        "\nNow let's see what it looks like:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "15 samples, 1 classes, 4 features.\n",
              "Class Cntrl : 15 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with updated description automatically, to indicate its history. Let's see some data from controls:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl.glance(2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": [
              "{'C0408': array([ 0.49,  0.38,  0.05,  0.82]),\n",
              " 'C0562': array([ 0.64,  0.59,  0.01,  0.8 ])}"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also query a random subset of samples for manual inspection or cross-validation purposes. For example:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "random_subset = dataset.random_subset(perc_in_class=0.3)\n",
        "random_subset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "12 samples, 3 classes, 4 features.\n",
              "Class Cntrl : 4 samples.\n",
              "Class Alzmr : 3 samples.\n",
              "Class   MCI : 5 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see which samples were selected:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "random_subset.sample_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": [
              "['C0562',\n",
              " 'C0565',\n",
              " 'C0372',\n",
              " 'C0492',\n",
              " 'A0240',\n",
              " 'A0032',\n",
              " 'A0229',\n",
              " 'M0034',\n",
              " 'M0770',\n",
              " 'M0552',\n",
              " 'M0165',\n",
              " 'M0127']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can verify that it is indeed random by issuing another call:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# supplying a new seed everytime to ensure randomization\n",
        "from datetime import datetime\n",
        "dataset.random_subset(perc_in_class=0.3).sample_ids"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": [
              "['C0562',\n",
              " 'C0822',\n",
              " 'C0949',\n",
              " 'C0141',\n",
              " 'A0034',\n",
              " 'A0141',\n",
              " 'A0032',\n",
              " 'M0434',\n",
              " 'M0942',\n",
              " 'M0868',\n",
              " 'M0421',\n",
              " 'M0552']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's see how we can retrieve specific samples by their IDs (for which there are many use cases):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset.get_subset(dataset.sample_ids[1:20])\n",
        "data"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "19 samples, 2 classes, 4 features.\n",
              "Class Cntrl : 14 samples.\n",
              "Class Alzmr : 5 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as simple as that."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation\n",
        "\nIf you would like to develop a variant of cross-validation, and need to obtain a random split of the dataset to obtain training and test sets, it is as simple as: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = dataset.train_test_split_ids( train_perc = 0.5)"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method returns two sets of sample ids corresponding to training set (which 50% of samples from all classes in the dataset) and the rest in test_set. Let's see what they have:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": [
              "(['C0760',\n",
              "  'C0822',\n",
              "  'C0565',\n",
              "  'C0170',\n",
              "  'C0562',\n",
              "  'C0141',\n",
              "  'C0041',\n",
              "  'A0768',\n",
              "  'A0888',\n",
              "  'A0032',\n",
              "  'A0969',\n",
              "  'A0141',\n",
              "  'A0034',\n",
              "  'M0434',\n",
              "  'M0421',\n",
              "  'M0577',\n",
              "  'M0256',\n",
              "  'M0127',\n",
              "  'M0033',\n",
              "  'M0760',\n",
              "  'M0476',\n",
              "  'M0165'],\n",
              " ['M0040',\n",
              "  'A0240',\n",
              "  'C0241',\n",
              "  'C0492',\n",
              "  'A0074',\n",
              "  'A0042',\n",
              "  'M0942',\n",
              "  'M0595',\n",
              "  'M0006',\n",
              "  'C0372',\n",
              "  'C0064',\n",
              "  'C0557',\n",
              "  'M0552',\n",
              "  'M0034',\n",
              "  'C0408',\n",
              "  'C0980',\n",
              "  'A0229',\n",
              "  'C0949',\n",
              "  'A0596',\n",
              "  'M0770',\n",
              "  'A0215',\n",
              "  'M0868',\n",
              "  'M0638'])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get a train/test split by specifying an exact number of subjects we would like from each class (e.g. when you would like to avoid class imbalance in the training set):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = dataset.train_test_split_ids( count_per_class = 3)"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the training set contains - we expect 3*3 =9 subjects :"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_set"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": [
              "['C0557',\n",
              " 'C0041',\n",
              " 'C0949',\n",
              " 'A0768',\n",
              " 'A0888',\n",
              " 'A0229',\n",
              " 'M0165',\n",
              " 'M0476',\n",
              " 'M0040']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can indeed verify that is the case, by creating a new smaller dataset from that list of ids and getting a summary:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = dataset.get_subset(train_set)\n",
        "training_dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "9 samples, 3 classes, 4 features.\n",
              "Class Cntrl : 3 samples.\n",
              "Class Alzmr : 3 samples.\n",
              "Class   MCI : 3 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another programmatic way to look into different classes is this:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class_set, label_set, class_sizes = training_dataset.summarize_classes()\n",
        "class_set, label_set, class_sizes"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": [
              "(['MCI', 'Cntrl', 'Alzmr'], [2, 0, 1], array([ 3.,  3.,  3.]))"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "which returns all the classes that you could iterative over."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these two lists, we can easily obtain subset datasets, as illustrated below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": [
              "ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "45 samples, 3 classes, 4 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples.\n",
              "Class   MCI : 18 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "binary_dataset = dataset.get_class(['Cntrl','Alzmr'])\n",
        "binary_dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 46,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "27 samples, 2 classes, 4 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 46,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about selecting a subset of features from all samples?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "binary_dataset.get_feature_subset(range(2))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": [
              "Subset features derived from: \n",
              " \n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "27 samples, 2 classes, 2 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 47,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Great.** Isn't it? You can also see the two-time-point history (initial subset in classes, followed by a subset in features)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='serialization'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serialization\n",
        "\nOnce you have this dataset, you can save and load these trivially using your favourite serialization module. Let's do some pickling:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "out_file = os.path.join(work_dir,'binary_dataset_Ctrl_Alzr_Freesurfer_thickness_v4p3.MLDataset.pkl')\n",
        "binary_dataset.save(out_file)"
      ],
      "outputs": [],
      "execution_count": 48,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it - it is saved.\n",
        "\nLet's reload it from disk and make sure we can indeed retrieve it:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = MLDataset(filepath=out_file) # another form of the constructor!"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": [
              "\n",
              " Subset derived from: ADNI1 baseline: cortical thickness features from Freesurfer v4.3, QCed.\n",
              "27 samples, 2 classes, 4 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check to see they are indeed one and the same:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "binary_dataset == reloaded"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 51,
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 51,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='arithmetic'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Arithmetic\n",
        "\n",
        "You might wonder how can you combine two different types of features ( thickness and shape ) from the dataset. Piece of cake, see below ...\n",
        "\nTo concatenat two datasets, first we make a second dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_two = MLDataset(in_dataset=dataset) # yet another constructor: in its copy form!"
      ],
      "outputs": [],
      "execution_count": 52,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can you check if they are \"functionally identical\"? As in same keys, same data and classes for each key... Easy:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_two == dataset"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try the arithmetic:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "combined = dataset + dataset_two"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identical keys found. Trying to horizontally concatenate features for each sample.\n"
          ]
        }
      ],
      "execution_count": 54,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great. The add method recognized the identical set of keys and performed a horiz cat, as can be noticed by the twice the number of features in the combined dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "combined"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": [
              "45 samples, 3 classes, 8 features.\n",
              "Class Cntrl : 15 samples.\n",
              "Class Alzmr : 12 samples.\n",
              "Class   MCI : 18 samples."
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do some removal in similar fashion:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "smaller = combined - dataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C0562 removed.\n",
            "C0408 removed.\n",
            "C0760 removed.\n",
            "C0170 removed.\n",
            "C0241 removed.\n",
            "C0980 removed.\n",
            "C0822 removed.\n",
            "C0565 removed.\n",
            "C0949 removed.\n",
            "C0041 removed.\n",
            "C0372 removed.\n",
            "C0141 removed.\n",
            "C0492 removed.\n",
            "C0064 removed.\n",
            "C0557 removed.\n",
            "A0034 removed.\n",
            "A0768 removed.\n",
            "A0240 removed.\n",
            "A0042 removed.\n",
            "A0141 removed.\n",
            "A0888 removed.\n",
            "A0032 removed.\n",
            "A0596 removed.\n",
            "A0969 removed.\n",
            "A0215 removed.\n",
            "A0074 removed.\n",
            "A0229 removed.\n",
            "M0760 removed.\n",
            "M0434 removed.\n",
            "M0033 removed.\n",
            "M0942 removed.\n",
            "M0034 removed.\n",
            "M0868 removed.\n",
            "M0595 removed.\n",
            "M0476 removed.\n",
            "M0770 removed.\n",
            "M0577 removed.\n",
            "M0638 removed.\n",
            "M0421 removed.\n",
            "M0006 removed.\n",
            "M0552 removed.\n",
            "M0040 removed.\n",
            "M0165 removed.\n",
            "M0256 removed.\n",
            "M0127 removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/Reddy/dev/pyradigm/pyradigm/pyradigm.py:1169: UserWarning: Requested removal of all the samples - output dataset would be empty.\n",
            "  warnings.warn('Requested removal of all the samples - output dataset would be empty.')\n"
          ]
        }
      ],
      "execution_count": 56,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data structure is even producing a warning to let you know the resulting output would be empty! We can verify that:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bool(smaller)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id='portability'></a>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portability"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is all well and good. How does it interact with other packages out there, you might ask? It is as simple as you can imagine:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma=0.001, C=100.)"
      ],
      "outputs": [],
      "execution_count": 58,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_matrix, target, sample_ids = binary_dataset.data_and_labels()\n",
        "clf.fit(data_matrix, target)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 59,
          "data": {
            "text/plain": [
              "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=False)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 59,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it, a simple example to show you the utility and convenience of this dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thanks for checking it out. \n",
        "\n### I would appreciate if you could give me feedback on improving or sharpening it further."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "py36",
      "language": "python",
      "display_name": "py36"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "py36"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}