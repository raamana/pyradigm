{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "\n",
    "* <a href='#motivation'>Motivation</a>\n",
    "\n",
    "* <a href='#constructor'>Constructing a dataset</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#access'>Accessing samples</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Saving/reloading a dataset (Serialization)</a>\n",
    "\n",
    "* <a href='#arithmetic'> Combining multiple datasets and arithmetic on useful subsets within datasets </a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research data management for medical and machine learning datasets with `pyradigm`\n",
    "\n",
    "Data structures provided by the pyradigm library are greatly suited for medical data, where each sample/subject/unit needs to be uniquely identified with a single identifier (denoted as subject ID or something similar depending on the field) that links multiple tables containing diverse types of data, such features extracted and prediction targets for machine learning analyses, covariates and demographics as well as needing to deal with multiple data types (numerical, categorical etc). \n",
    "\n",
    "Key-level correspondence across data, targets (either as labels 1 or 2, or as class names such as `healthy` and `disease`) and related tables (demographics and other meta data) is essential to maintain integrity of the datasets themselves as well as the subsequent analyses. This would help improve the provenance, as it is easy to encapsulate the relevant info and necessary history in the user-defined attribute and meta data features.\n",
    "\n",
    "To provide a concrete examples, let's look at how a machine learning dataset is handled traditionally.\n",
    "\n",
    "You typically have a matrix `X` of size `n x p` (`n` samples and `p` features), and a target vector `y` containing the target values (numerical or categorical or multi-output arrays). These `X` and `y` serve as training (and test set) for a classifier (like SVM) to fit the data `X` to match `y` as accurately as possible.\n",
    "\n",
    "Let's get a little more concrete, and produce toy data for `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "X : \n",
      "[[0.36 0.81 0.75]\n",
      " [0.32 0.19 0.9 ]\n",
      " [0.17 0.72 0.04]\n",
      " [0.84 0.66 0.07]\n",
      " [0.41 0.41 0.04]\n",
      " [0.27 0.96 0.65]\n",
      " [0.53 0.06 0.89]\n",
      " [0.04 0.44 0.46]\n",
      " [0.6  0.05 0.42]\n",
      " [0.5  0.06 0.58]]\n",
      "y : \n",
      "[1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10 # number of samples\n",
    "p = 3  # number of features\n",
    "\n",
    "X = np.random.random([n, p]) # random data for illustration\n",
    "y = [1]*5 + [2]*5            # random labels ...\n",
    "\n",
    "np.set_printoptions(precision=2) # save some screen space\n",
    "print('X : \\n{}'.format(X))\n",
    "print('y : \\n{}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the machine learning toolboxes take their input in this form: X and y, regardless of the original source that produced these features in the first place.\n",
    "\n",
    "This might be fine if all you ever wanted to do is to extract some features, do some machine learning one-time and dispose these features away! \n",
    "\n",
    "**But this is almost never the case!**\n",
    "\n",
    "Because it doesn't simply end there.\n",
    "\n",
    "At a minimum, you often need to dissect the results and link them across different tables e.g. to figure out  \n",
    " * which samples are misclassified, that can only be queried with their identifiers and not simply their row indices in X?\n",
    " * what are the characteristics of those samples?\n",
    " * what targets/labels/classes do they belong to?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all this info needs to be obtained\n",
    " * without having to write lots of code connecting few non-obvious links to disparate sources of data (numerical features X, and sample identifiers in a CSV file) to find the relevant info\n",
    " * without having to track down who or which method originally produced these features\n",
    " * how the previous colleage or student organized the whole dataset, if you haven't generated the features yourself from scratch\n",
    "\n",
    "And if you are like me, you would be thinking about how would you organize your workflow such that the aforementioned tasks can be accomplished with ease.\n",
    " \n",
    "This pyradigm data structure is the result of years of refinement to conquer the above complex workflow and related scenarios with ease. By always organizing the extracted features in a dictionary with keyed-in by their *samplet* ID, along with other important info such as *target values*, other attributes and associated meta-data. This, by definition, preserves the integrity of the data (inability to incorrectly label samples etc).\n",
    "\n",
    "With user-defined attributes, this data structure allows the capture of provenance info in a way that is meaningful to the user.\n",
    "\n",
    "NOTE: we define a *samplet* to be a single row in `X`, to distinguish from a `sample` that is sometimes incorrectly used to refer both a single instance and an entire \"sample\" / dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example application is shown below, touching upon the following topics:\n",
    "\n",
    "* <a href='#motivation'>Motivation</a>\n",
    "\n",
    "* <a href='#constructor'>Constructing a dataset</a>\n",
    "\n",
    "* <a href='#attributes'>Attributes</a>\n",
    "\n",
    "* <a href='#access'>Accessing samples</a>\n",
    "\n",
    "* <a href='#iteration'>Iteration over samples</a>\n",
    "\n",
    "* <a href='#subsetselection'>Subset selection</a>\n",
    "\n",
    "* <a href='#serialization'>Saving/reloading a dataset (Serialization)</a>\n",
    "\n",
    "* <a href='#arithmetic'> Combining multiple datasets and arithmetic on useful subsets within datasets </a>\n",
    "\n",
    "* <a href='#portability'>Portability (e.g. with sklearn)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improting the necessary modules and our fancy class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyradigm import ClassificationDataset as ClfDataset\n",
    "from pyradigm import RegressionDataset as RegrDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only major difference between the above two data structures is the data type of target i.e. categorical/discrete vs. continuous values. The ClassificationDataset accepts only categorical values for its target, often as strings ('healthy', 'disease', 'cat', 'chair' etc), whereas the RegressionDataset allows continuous floating point numbers (e.g. age, temperature, salary, weight etc). Depending on the target data type, the analyses (esp. in machine learning) changes dramatically in terms of which predictive model is employed, how they are optimized and what performance metrics are considered etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started with a classification dataset, which can be instantiated as shown below. We also give it a simple description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClfDataset()\n",
    "dataset.description = 'ADNI1: cortical thickness features from Freesurfer v6.0, QCed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dataset have customised and convenient display methods, summarizing the important info. For example, just printing instance shows its content summary (typically count of samplets per target and a description if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "Empty dataset."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the dataset some description attached to it, however we can also it is empty. This can be verified in a boolean context as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's add samples to this dataset which is when this dataset implementation becomes really handy. Before we do that, we will define some convenience routines defined to just illustrate a simple yet common use of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_thickness(path):\n",
    "    \"\"\"Dummy function to minic a data reader.\"\"\"\n",
    "\n",
    "    # in your actural routine, this might be:\n",
    "    #   pysurfer.read_thickness(path).values()\n",
    "    return np.random.random(2)\n",
    "\n",
    "\n",
    "def get_features(work_dir, subj_id):\n",
    "    \"\"\"Returns the whole brain cortical thickness for a given subject ID.\"\"\"\n",
    "\n",
    "    # extension to identify the data file; this could be .curv, anything else you choose\n",
    "    ext_thickness = '.thickness'\n",
    "\n",
    "    thickness = dict()\n",
    "    for hemi in ['lh', 'rh']:\n",
    "        path_thickness = os.path.join(work_dir, subj_id, hemi + ext_thickness)\n",
    "        thickness[hemi] = read_thickness(path_thickness)\n",
    "\n",
    "    # concatenating them to build a whole brain feature set\n",
    "    thickness_wb = np.concatenate([thickness['lh'], thickness['rh']])\n",
    "\n",
    "    return thickness_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have IO routines to read the data for us. Let's define where the data will come from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '/project/ADNI/FreesurferThickness_v4p3'\n",
    "class_set = ['Control', 'Alzheimer', 'MCI']\n",
    "class_sizes = [7, 8, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would obviously change for your applications, but this has sufficient properties to illustrate the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what methods this dataset offers us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add_attr',\n",
       " 'add_dataset_attr',\n",
       " 'add_samplet',\n",
       " 'attr',\n",
       " 'attr_dtype',\n",
       " 'attr_summary',\n",
       " 'data',\n",
       " 'data_and_labels',\n",
       " 'data_and_targets',\n",
       " 'dataset_attr',\n",
       " 'del_attr',\n",
       " 'del_samplet',\n",
       " 'description',\n",
       " 'dtype',\n",
       " 'extend',\n",
       " 'feature_names',\n",
       " 'from_arff',\n",
       " 'get',\n",
       " 'get_attr',\n",
       " 'get_class',\n",
       " 'get_data_matrix_in_order',\n",
       " 'get_feature_subset',\n",
       " 'get_subset',\n",
       " 'glance',\n",
       " 'num_features',\n",
       " 'num_samplets',\n",
       " 'num_targets',\n",
       " 'random_subset',\n",
       " 'random_subset_ids',\n",
       " 'random_subset_ids_by_count',\n",
       " 'rename_targets',\n",
       " 'sample_ids_in_class',\n",
       " 'samplet_ids',\n",
       " 'save',\n",
       " 'shape',\n",
       " 'summarize',\n",
       " 'target_set',\n",
       " 'target_sizes',\n",
       " 'targets',\n",
       " 'train_test_split_ids',\n",
       " 'transform']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mm for mm in dir(dataset) if not mm.startswith('_') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of methods and attributes to use, organize and retrieve datasets. \n",
    "\n",
    "So let's go through them by their usage sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructor\n",
    "\n",
    "You can see there few methods such as `.add_samplet()`, `.get_subset()` etc. The most often used method is the `.add_samplet()`, which is key to constructing a pyradigm dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To contruct a dataset, one typically starts with a list of subject IDs to be added - we create few random lists, each to be considered as a separate class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def get_id_list(class_name, size=10):\n",
    "    \"\"\"Generates a random ID list. \"\"\"\n",
    "        \n",
    "    return ['{}{:04d}'.format(class_name[0],np.random.randint(50*size)) for _ in range(size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be populated by adding all subjects belonging to a one class (referred to by `cls_id` here), done by adding one samplet at a time, using the `.add_samplet()` method. Let's go ahead and add some samplets based on id lists we just created.\n",
    "\n",
    "*Note*: `samplet` here refers to a single in the sample feature matrix X. This new term `samplet` is defined to distinguish individual row elements of X from X itself and minimized confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding class Control\n",
      "\t reading subject   C0052\n",
      "\t reading subject   C0101\n",
      "\t reading subject   C0166\n",
      "\t reading subject   C0168\n",
      "\t reading subject   C0018\n",
      "\t reading subject   C0267\n",
      "\t reading subject   C0000\n",
      "Adding class Alzheimer\n",
      "\t reading subject   A0179\n",
      "\t reading subject   A0253\n",
      "\t reading subject   A0152\n",
      "\t reading subject   A0164\n",
      "\t reading subject   A0141\n",
      "\t reading subject   A0320\n",
      "\t reading subject   A0197\n",
      "\t reading subject   A0189\n",
      "Adding class   MCI\n",
      "\t reading subject   M0269\n",
      "\t reading subject   M0078\n",
      "\t reading subject   M0298\n",
      "\t reading subject   M0024\n",
      "\t reading subject   M0022\n",
      "\t reading subject   M0081\n"
     ]
    }
   ],
   "source": [
    "for class_index, class_id in enumerate(class_set):\n",
    "    print('Adding class {:>5}'.format(class_id))\n",
    "\n",
    "    target_list = get_id_list(class_id,class_sizes[class_index])\n",
    "    for subj_id in target_list:\n",
    "        print('\\t reading subject {:>7}'.format(subj_id))\n",
    "        thickness_wb = get_features(work_dir, subj_id)\n",
    "\n",
    "        # adding the sample to the dataset\n",
    "        dataset.add_samplet(subj_id, thickness_wb, class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice. Isn't it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's nice about this, you say? *The simple fact that you are constructing a dataset as you read the data* in its most elemental form (in the units of the dataset such as the subject ID in our neuroimaging application). You're done as soon as you're done reading the features from disk.\n",
    "\n",
    "What's more - you can inspect the dataset in an intuitive manner, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "21 samplets, 3 classes, 4 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets\n",
       "Class       MCI : 6 samplets"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, right? No more coding of several commands to get the complete and concise sense of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='attributes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenient attributes\n",
    "\n",
    "If you would like, you can always get more specific information, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_samplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MCI', 'Control', 'Alzheimer']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Control': 7, 'Alzheimer': 8, 'MCI': 6})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_sizes['Control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to take a look data inside for few subjects - shall we call it a glance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0052': array([0.27, 0.94, 0.06, 0.06]),\n",
       " 'C0101': array([0.02, 0.92, 0.53, 0.72]),\n",
       " 'C0166': array([0.44, 0.47, 0.64, 0.17]),\n",
       " 'C0168': array([0.46, 0.83, 0.38, 0.62]),\n",
       " 'C0018': array([0.3 , 0.95, 0.84, 0.73])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the number of items to glance, by passing a number to dataset.glance() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0052': array([0.27, 0.94, 0.06, 0.06]),\n",
       " 'C0101': array([0.02, 0.92, 0.53, 0.72])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you may be wondering what are the subject IDs in the dataset.. here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0052',\n",
       " 'C0101',\n",
       " 'C0166',\n",
       " 'C0168',\n",
       " 'C0018',\n",
       " 'C0267',\n",
       " 'C0000',\n",
       " 'A0179',\n",
       " 'A0253',\n",
       " 'A0152',\n",
       " 'A0164',\n",
       " 'A0141',\n",
       " 'A0320',\n",
       " 'A0197',\n",
       " 'A0189',\n",
       " 'M0269',\n",
       " 'M0078',\n",
       " 'M0298',\n",
       " 'M0024',\n",
       " 'M0022',\n",
       " 'M0081']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.samplet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets offer all convenient methods and attributes you need. Besides it is quite easy to extend them to fit your needs and improve your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='access'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing samples\n",
    "\n",
    "Thanks to its design, data for a given samplet 'M0299' can simply be obtained by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31, 0.86, 0.7 , 0.01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['M0022']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a Python dict, it raises an error if the key is not in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dlfjdjf not found in dataset.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4b19d52bac71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dlfjdjf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/pyradigm/pyradigm/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} not found in dataset.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dlfjdjf not found in dataset.'"
     ]
    }
   ],
   "source": [
    "dataset['dlfjdjf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more graceful handling would be to use `dataset.get` to control what value to be returned in case the requested id is not found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get('dkfjd', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iteration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to builtin iteration, we can easily iterate over all the samplets in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3c3646e8c9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamplet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} : {:>10} : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamplet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "for samplet, features in dataset:\n",
    "    print(\"{} : {:>10} : {}\".format(sample, dataset.targets[samplet], features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you see that? *It's so intuitive and natural!* Such a clean traversal of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the choice of the OrderedDict() to represent the data, classes and labels underneath, the order of sample addition is retained. Hence the correspondence across samples in the dataset not only key-wise (by the sample id), but also index-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transform'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject-wise transform\n",
    "\n",
    "Quite often, we are interested in computing some statistics on data for a given subject (such as mean, or ROI-wise median). Typically this requires a loop, with some computation and organizing it in a new dataset! A simple routine pattern of usage, but can't avoided if you are still fiddling with representing your dataset in medieval matrices! :).\n",
    "\n",
    "If you organized your dataset in a `pyradigm`, such computation is trivial, thanks to builtin implementation of `transform` method. The mean value for each subject can be computed and organized in a new dataset, with an intuitive and single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_data = dataset.transform(np.mean)\n",
    "mean_data.description = 'mean values per subject'\n",
    "mean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can easily traverse the dataset to check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for samplet, val in mean_data:\n",
    "    print(\"{} : {:>10} : {:.3f}\".format(samplet, mean_data.targets[samplet], val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the transform accepts an arbitrary callable, we could do many more sophisticated things, such as access the subset of features e.g. cortical thickness for a particular region of interest (say posterior cingulate gyrus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a toy function to return the indices for the ROI\n",
    "def get_ROI_indices(x): return x[:3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this \"mask\" function, we can easily obtain features for an ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg = dataset.transform(get_ROI_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the new dataset does indeed have only 3 features, for the same subjects/classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcg.num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bar plot with the just computed numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lbl, keys = pcg.data_and_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember as the original source of data was random, this has no units, property or meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subsetselection'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset selection\n",
    "\n",
    "In addition to the structured way of obtaining the various properties of this dataset, this implementation really will come in handy when you have to slice and dice the dataset (with large number of classes and features) into smaller subsets (e.g. for binary classification). Let's see how we can retrieve the data for a single class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = dataset.get_class('Control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, obtaining the data for a given class is a simple call away.\n",
    "\n",
    "Now let's see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "7 samplets, 1 classes, 4 features\n",
       "Class Control : 7 samplets"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with updated description automatically, to indicate its history. Let's see some data from controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C0052': array([0.27, 0.94, 0.06, 0.06]),\n",
       " 'C0101': array([0.02, 0.92, 0.53, 0.72])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrl.glance(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query a random subset of samples for manual inspection or cross-validation purposes. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "5 samplets, 3 classes, 4 features\n",
       "Class   Control : 2 samplets\n",
       "Class Alzheimer : 2 samplets\n",
       "Class       MCI : 1 samplets"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_subset = dataset.random_subset(perc_in_class=0.3)\n",
    "random_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see which samplets were selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0101', 'C0168', 'A0253', 'A0164', 'M0078']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_subset.samplet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that it is indeed random by issuing another call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0168', 'C0267', 'A0253', 'A0320', 'M0022']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supplying a new seed everytime to ensure randomization\n",
    "from datetime import datetime\n",
    "dataset.random_subset(perc_in_class=0.3).samplet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how we can retrieve specific samples by their IDs (for which there are many use cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "19 samplets, 3 classes, 4 features\n",
       "Class   Control : 6 samplets\n",
       "Class Alzheimer : 8 samplets\n",
       "Class       MCI : 5 samplets"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.get_subset(dataset.samplet_ids[1:20])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as simple as that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation\n",
    "\n",
    "If you would like to develop a variant of cross-validation, and need to obtain a random split of the dataset to obtain training and test sets, it is as simple as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.train_test_split_ids( train_perc = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns two sets of sample ids corresponding to training set (which 50% of samples from all classes in the dataset) and the rest in test_set. Let's see what they have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['C0018',\n",
       "  'C0166',\n",
       "  'C0101',\n",
       "  'A0152',\n",
       "  'A0189',\n",
       "  'A0253',\n",
       "  'A0320',\n",
       "  'M0078',\n",
       "  'M0269',\n",
       "  'M0024'],\n",
       " ['A0197',\n",
       "  'M0081',\n",
       "  'A0141',\n",
       "  'C0000',\n",
       "  'C0168',\n",
       "  'M0298',\n",
       "  'A0164',\n",
       "  'C0052',\n",
       "  'A0179',\n",
       "  'M0022',\n",
       "  'C0267'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a train/test split by specifying an exact number of subjects we would like from each class (e.g. when you would like to avoid class imbalance in the training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.train_test_split_ids( count_per_class = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the training set contains - we expect 3*3 =9 subjects :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C0166',\n",
       " 'C0018',\n",
       " 'C0168',\n",
       " 'A0320',\n",
       " 'A0141',\n",
       " 'A0253',\n",
       " 'M0024',\n",
       " 'M0081',\n",
       " 'M0269']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indeed verify that is the case, by creating a new smaller dataset from that list of ids and getting a summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "9 samplets, 3 classes, 4 features\n",
       "Class   Control : 3 samplets\n",
       "Class Alzheimer : 3 samplets\n",
       "Class       MCI : 3 samplets"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = dataset.get_subset(train_set)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another programmatic way to look into different classes is this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MCI', 'Control', 'Alzheimer'], array([3., 3., 3.]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_set, target_sizes = training_dataset.summarize()\n",
    "target_set, target_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which returns all the classes that you could iterative over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these two lists, we can easily obtain subset datasets, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "21 samplets, 3 classes, 4 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets\n",
       "Class       MCI : 6 samplets"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "15 samplets, 2 classes, 4 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset = dataset.get_class(['Control','Alzheimer'])\n",
    "binary_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about selecting a subset of features from all samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Subset features derived from: \n",
       " \n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "15 samplets, 2 classes, 2 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset.get_feature_subset(range(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great.** Isn't it? You can also see the two-time-point history (initial subset in classes, followed by a subset in features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='serialization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization\n",
    "\n",
    "Once you have this dataset, you can save and load these trivially using your favourite serialization module. Let's do some pickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out_file = Path('.') / 'Freesurfer_thickness_v4p3.PyradigmDataset.pkl'\n",
    "binary_dataset.save(out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - it is saved.\n",
    "\n",
    "Let's reload it from disk and make sure we can indeed retrieve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = ClfDataset(out_file) # another form of the constructor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " Subset derived from: ADNI1: cortical thickness features from Freesurfer v6.0, QCed.\n",
       "15 samplets, 2 classes, 4 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see they are indeed one and the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dataset == reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arithmetic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Arithmetic\n",
    "\n",
    "You might wonder how can you combine two different types of features ( thickness and shape ) from the dataset. Piece of cake, see below ...\n",
    "\n",
    "To concatenat two datasets, first we make a second dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_two = ClfDataset(in_dataset=dataset) # yet another constructor: in its copy form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you check if they are \"functionally identical\"? As in same keys, same data and classes for each key... Easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_two == dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical keys found. Trying to horizontally concatenate features for each samplet.\n"
     ]
    }
   ],
   "source": [
    "combined = dataset + dataset_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. The add method recognized the identical set of keys and performed a horiz cat, as can be noticed by the twice the number of features in the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21 samplets, 3 classes, 8 features\n",
       "Class   Control : 7 samplets\n",
       "Class Alzheimer : 8 samplets\n",
       "Class       MCI : 6 samplets"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some removal in similar fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C0052 removed.\n",
      "C0101 removed.\n",
      "C0166 removed.\n",
      "C0168 removed.\n",
      "C0018 removed.\n",
      "C0267 removed.\n",
      "C0000 removed.\n",
      "A0179 removed.\n",
      "A0253 removed.\n",
      "A0152 removed.\n",
      "A0164 removed.\n",
      "A0141 removed.\n",
      "A0320 removed.\n",
      "A0197 removed.\n",
      "A0189 removed.\n",
      "M0269 removed.\n",
      "M0078 removed.\n",
      "M0298 removed.\n",
      "M0024 removed.\n",
      "M0022 removed.\n",
      "M0081 removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Reddy/dev/pyradigm/pyradigm/base.py:1470: UserWarning: Requested removal of all the samplets - output dataset would be empty.\n",
      "  warn('Requested removal of all the samplets - '\n"
     ]
    }
   ],
   "source": [
    "smaller = combined - dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structure is even producing a warning to let you know the resulting output would be empty! We can verify that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(smaller)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='portability'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good. How does it interact with other packages out there, you might ask? It is as simple as you can imagine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix, target, sample_ids = binary_dataset.data_and_targets()\n",
    "clf.fit(data_matrix, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, a simple example to show you the utility and convenience of this `ClassificationDataset`. \n",
    "\n",
    "Usage for the `RegressionDataset` will more or less be the same, except keeping in mind that the target value is continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for checking it out. \n",
    "\n",
    "### I would appreciate if you could give me feedback on improving or sharpening it further."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "py36"
  },
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
